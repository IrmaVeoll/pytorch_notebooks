{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "pytorch_mnist_mini_es_rank_oes.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IrmaVeoll/pytorch_notebooks/blob/master/mnist_es/pytorch_mnist_mini_es_rank_oes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L9bCG8z8AMA"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import multiprocessing as mp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from collections import namedtuple\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import errno\n",
        "import codecs\n",
        "import copy"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkPfnuqn8AMd"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "print(\"torch.cuda.device_count()\", torch.cuda.device_count())\n",
        "print(\"torch.cuda.current_device()\", torch.cuda.current_device())\n",
        "torch.cuda.set_device(0)\n",
        "print(\"torch.cuda.current_device()\", torch.cuda.current_device())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01lw8PIo8AM4"
      },
      "source": [
        "def compute_ranks(x):\n",
        "  \"\"\"\n",
        "  Returns ranks in [0, len(x))\n",
        "  Note: This is different from scipy.stats.rankdata, which returns ranks in [1, len(x)].\n",
        "  (https://github.com/openai/evolution-strategies-starter/blob/master/es_distributed/es.py)\n",
        "  \"\"\"\n",
        "  assert x.ndim == 1\n",
        "  ranks = np.empty(len(x), dtype=int)\n",
        "  ranks[x.argsort()] = np.arange(len(x))\n",
        "  return ranks\n",
        "\n",
        "def compute_centered_ranks(x):\n",
        "  \"\"\"\n",
        "  https://github.com/openai/evolution-strategies-starter/blob/master/es_distributed/es.py\n",
        "  \"\"\"\n",
        "  y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
        "  y /= (x.size - 1)\n",
        "  y -= .5\n",
        "  return y\n",
        "\n",
        "def compute_weight_decay(weight_decay, model_param_list):\n",
        "  model_param_grid = np.array(model_param_list)\n",
        "  return - weight_decay * np.mean(model_param_grid * model_param_grid, axis=1)\n",
        "\n",
        "class CMAES:\n",
        "  '''CMA-ES wrapper.'''\n",
        "  def __init__(self, num_params,      # number of model parameters\n",
        "               sigma_init=0.10,       # initial standard deviation\n",
        "               popsize=255):          # population size\n",
        "\n",
        "    self.num_params = num_params\n",
        "    self.sigma_init = sigma_init\n",
        "    self.popsize = popsize\n",
        "\n",
        "    self.solutions = None\n",
        "\n",
        "    import cma\n",
        "    self.es = cma.CMAEvolutionStrategy( self.num_params * [0],\n",
        "                                        self.sigma_init,\n",
        "                                        {'popsize': self.popsize})\n",
        "\n",
        "  def rms_stdev(self):\n",
        "    sigma = self.es.result()[6]\n",
        "    return np.mean(np.sqrt(sigma*sigma))\n",
        "\n",
        "  def ask(self):\n",
        "    '''returns a list of parameters'''\n",
        "    self.solutions = np.array(self.es.ask())\n",
        "    return self.solutions\n",
        "\n",
        "  def tell(self, reward_table_result):\n",
        "    reward_table = reward_table_result\n",
        "    self.es.tell(self.solutions, (-reward_table).tolist()) # convert minimizer to maximizer.\n",
        "\n",
        "  def done(self):\n",
        "    return self.es.stop()\n",
        "\n",
        "  def current_param(self):\n",
        "    return self.es.result()[5] # mean solution, presumably better with noise\n",
        "  \n",
        "  def best_param(self):\n",
        "    return self.es.result()[0] # best evaluated solution\n",
        "\n",
        "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
        "    r = self.es.result()\n",
        "    return (r[0], -r[1], -r[1], r[6])\n",
        "\n",
        "class SimpleES:\n",
        "  '''Simple Evolution Strategies.'''\n",
        "  def __init__(self, num_params,      # number of model parameters\n",
        "               sigma_init=0.10,       # initial standard deviation\n",
        "               sigma_alpha=0.20,      # learning rate for standard deviation\n",
        "               sigma_decay=0.999,     # anneal standard deviation\n",
        "               sigma_limit=0.01,      # stop annealing if less than this\n",
        "               popsize=255,           # population size\n",
        "               elite_ratio=0.1,       # percentage of the elites\n",
        "               done_threshold=1e-6,   # threshold when we say we are done\n",
        "               average_baseline=True, # set baseline to average of batch\n",
        "               forget_best=True):     # only use the best from latest generation\n",
        "\n",
        "    self.num_params = num_params\n",
        "    self.sigma_init = sigma_init\n",
        "    self.sigma_alpha = sigma_alpha\n",
        "    self.sigma_decay = sigma_decay\n",
        "    self.sigma_limit = sigma_limit\n",
        "    self.popsize = popsize\n",
        "    self.average_baseline = average_baseline\n",
        "    if self.average_baseline:\n",
        "      assert (self.popsize & 2), \"Population size must be even\"\n",
        "      self.batch_size = int(self.popsize / 2)\n",
        "    else:\n",
        "      assert (self.popsize & 1), \"Population size must be odd\"\n",
        "      self.batch_size = int((self.popsize - 1) / 2)\n",
        "    self.elite_ratio = elite_ratio\n",
        "    self.elite_popsize = int(self.popsize * self.elite_ratio)\n",
        "    self.forget_best = forget_best\n",
        "    self.batch_reward = np.zeros(self.batch_size * 2)\n",
        "    self.mu = np.zeros(self.num_params)\n",
        "    self.sigma = np.ones(self.num_params) * self.sigma_init\n",
        "    self.curr_best_mu = np.zeros(self.num_params)\n",
        "    self.best_mu = np.zeros(self.num_params)\n",
        "    self.best_reward = 0\n",
        "    self.first_interation = True\n",
        "    self.done_threshold = done_threshold\n",
        "\n",
        "  def rms_stdev(self):\n",
        "    sigma = self.sigma\n",
        "    return np.mean(np.sqrt(sigma*sigma))\n",
        "\n",
        "  def ask(self):\n",
        "    '''returns a list of parameters'''\n",
        "    # antithetic sampling\n",
        "    self.epsilon = np.random.randn(self.batch_size, self.num_params) * self.sigma.reshape(1, self.num_params)\n",
        "    self.epsilon_full = np.concatenate([self.epsilon, - self.epsilon])\n",
        "    if self.average_baseline:\n",
        "      epsilon = self.epsilon_full\n",
        "    else:\n",
        "      # first population is mu, then positive epsilon, then negative epsilon\n",
        "      epsilon = np.concatenate([np.zeros((1, self.num_params)), self.epsilon_full])\n",
        "    solutions = self.mu.reshape(1, self.num_params) + epsilon\n",
        "    return solutions\n",
        "\n",
        "  def tell(self, reward_table_result):\n",
        "    # input must be a numpy float array\n",
        "    assert(len(reward_table_result) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
        "\n",
        "    reward_table = reward_table_result\n",
        "\n",
        "    reward_offset = 1\n",
        "    if self.average_baseline:\n",
        "      b = np.mean(reward_table)\n",
        "      reward_offset = 0\n",
        "    else:\n",
        "      b = reward_table[0] # baseline\n",
        "      \n",
        "    reward = reward_table[reward_offset:]\n",
        "    idx = np.argsort(reward)[::-1][0:self.elite_popsize]\n",
        "\n",
        "    best_reward = reward[idx[0]]\n",
        "    if (best_reward > b or self.average_baseline):\n",
        "      best_mu = self.mu + self.epsilon_full[idx[0]]\n",
        "      best_reward = reward[idx[0]]\n",
        "    else:\n",
        "      best_mu = self.mu\n",
        "      best_reward = b\n",
        "\n",
        "    self.curr_best_reward = best_reward\n",
        "    self.curr_best_mu = best_mu\n",
        "\n",
        "    if self.first_interation:\n",
        "      self.first_interation = False\n",
        "      self.best_reward = self.curr_best_reward\n",
        "      self.best_mu = best_mu\n",
        "    else:\n",
        "      if self.forget_best or (self.curr_best_reward > self.best_reward):\n",
        "        self.best_mu = best_mu\n",
        "        self.best_reward = self.curr_best_reward\n",
        "\n",
        "    # adaptive sigma\n",
        "    # normalization\n",
        "    stdev_reward = reward.std()\n",
        "    epsilon = self.epsilon\n",
        "    sigma = self.sigma\n",
        "    S = ((epsilon * epsilon - (sigma * sigma).reshape(1, self.num_params)) / sigma.reshape(1, self.num_params))\n",
        "    reward_avg = (reward[:self.batch_size] + reward[self.batch_size:]) / 2.0\n",
        "    rS = reward_avg - b\n",
        "    delta_sigma = (np.dot(rS, S)) / (2 * self.batch_size * stdev_reward)\n",
        "\n",
        "    # move mean to the average of the best idx means\n",
        "    self.mu += self.epsilon_full[idx].mean(axis=0)\n",
        "\n",
        "    # adjust sigma according to the adaptive sigma calculation\n",
        "    change_sigma = self.sigma_alpha * delta_sigma\n",
        "    change_sigma = np.minimum(change_sigma, self.sigma)\n",
        "    change_sigma = np.maximum(change_sigma, - 0.5 * self.sigma)\n",
        "    self.sigma += change_sigma\n",
        "    self.sigma[self.sigma > self.sigma_limit] *= self.sigma_decay\n",
        "\n",
        "  def done(self):\n",
        "    return (self.rms_stdev() < self.done_threshold)\n",
        "\n",
        "  def current_param(self):\n",
        "    return self.curr_best_mu\n",
        "  \n",
        "  def best_param(self):\n",
        "    return self.best_mu\n",
        "\n",
        "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
        "    return (self.best_mu, self.best_reward, self.curr_best_reward, self.sigma)\n",
        "\n",
        "class SimpleGA:\n",
        "  '''Simple Genetic Algorithm.'''\n",
        "  def __init__(self, num_params,      # number of model parameters\n",
        "               sigma_init=0.1,        # initial standard deviation\n",
        "               sigma_decay=0.999,     # anneal standard deviation\n",
        "               sigma_limit=0.01,      # stop annealing if less than this\n",
        "               popsize=255,           # population size\n",
        "               elite_ratio=0.1,       # percentage of the elites\n",
        "               forget_best=True,      # forget the historical best elites\n",
        "               done_threshold=1e-6):  # threshold when we say we are done\n",
        "\n",
        "    self.num_params = num_params\n",
        "    self.sigma_init = sigma_init\n",
        "    self.sigma_decay = sigma_decay\n",
        "    self.sigma_limit = sigma_limit\n",
        "    self.popsize = popsize\n",
        "\n",
        "    self.elite_ratio = elite_ratio\n",
        "    self.elite_popsize = int(self.popsize * self.elite_ratio)\n",
        "\n",
        "    self.sigma = self.sigma_init\n",
        "    self.elite_params = np.zeros((self.elite_popsize, self.num_params))\n",
        "    self.elite_rewards = np.zeros(self.elite_popsize)\n",
        "    self.best_param = np.zeros(self.num_params)\n",
        "    self.best_reward = 0\n",
        "    self.first_iteration = True\n",
        "    self.forget_best = forget_best\n",
        "    self.done_threshold = done_threshold\n",
        "\n",
        "  def rms_stdev(self):\n",
        "    return self.sigma # same sigma for all parameters.\n",
        "\n",
        "  def ask(self):\n",
        "    '''returns a list of parameters'''\n",
        "    # antithetic sampling\n",
        "    self.epsilon = np.random.randn(self.popsize, self.num_params) * self.sigma\n",
        "    solutions = []\n",
        "    \n",
        "    def mate(a, b):\n",
        "      c = np.copy(a)\n",
        "      idx = np.where(np.random.rand((c.size)) > 0.5)\n",
        "      c[idx] = b[idx]\n",
        "      return c\n",
        "    \n",
        "    elite_range = range(self.elite_popsize)\n",
        "    for i in range(self.popsize):\n",
        "      idx_a = np.random.choice(elite_range)\n",
        "      idx_b = np.random.choice(elite_range)\n",
        "      child_params = mate(self.elite_params[idx_a], self.elite_params[idx_b])\n",
        "      solutions.append(child_params + self.epsilon[i])\n",
        "\n",
        "    solutions = np.array(solutions)\n",
        "    self.solutions = solutions\n",
        "\n",
        "    return solutions\n",
        "\n",
        "  def tell(self, reward_table_result):\n",
        "    # input must be a numpy float array\n",
        "    assert(len(reward_table_result) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
        "    \n",
        "    if (not forget_best or self.first_iteration):\n",
        "      reward = reward_table_result\n",
        "      solution = self.solutions\n",
        "    else:\n",
        "      reward = np.concatenate([reward_table_result, self.elite_rewards])\n",
        "      solution = np.concatenate([self.solutions, self.elite_params])\n",
        "\n",
        "    idx = np.argsort(reward)[::-1][0:self.elite_popsize]\n",
        "\n",
        "    self.elite_rewards = reward[idx]\n",
        "    self.elite_params = solution[idx]\n",
        "\n",
        "    self.curr_best_reward = self.elite_rewards[0]\n",
        "    \n",
        "    if self.first_iteration or (self.curr_best_reward > self.best_reward):\n",
        "      self.first_iteration = False\n",
        "      self.best_reward = self.elite_rewards[0]\n",
        "      self.best_param = np.copy(self.elite_params[0])\n",
        "\n",
        "    if (self.sigma > self.sigma_limit):\n",
        "      self.sigma *= self.sigma_decay\n",
        "\n",
        "  def done(self):\n",
        "    return (self.rms_stdev() < self.done_threshold)\n",
        "\n",
        "  def current_param(self):\n",
        "    return self.elite_params[0]\n",
        "\n",
        "  def best_param(self):\n",
        "    return self.best_param\n",
        "\n",
        "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
        "    return (self.best_param, self.best_reward, self.curr_best_reward, self.sigma)\n",
        "\n",
        "class OpenES:\n",
        "  ''' Basic Version of OpenAI Evolution Strategies.'''\n",
        "  def __init__(self, num_params,             # number of model parameters\n",
        "               sigma_init=0.1,               # initial standard deviation\n",
        "               sigma_decay=0.999,            # anneal standard deviation\n",
        "               sigma_limit=0.01,             # stop annealing if less than this\n",
        "               learning_rate=0.001,          # learning rate for standard deviation\n",
        "               learning_rate_decay = 0.9999, # annealing the learning rate\n",
        "               learning_rate_limit = 0.001,  # stop annealing learning rate\n",
        "               popsize=255,                  # population size\n",
        "               antithetic=False,             # whether to use antithetic sampling\n",
        "               forget_best=True):           # forget historical best\n",
        "\n",
        "    self.num_params = num_params\n",
        "    self.sigma_decay = sigma_decay\n",
        "    self.sigma = sigma_init\n",
        "    self.sigma_limit = sigma_limit\n",
        "    self.learning_rate = learning_rate\n",
        "    self.learning_rate_decay = learning_rate_decay\n",
        "    self.learning_rate_limit = learning_rate_limit\n",
        "    self.popsize = popsize\n",
        "    self.antithetic = antithetic\n",
        "    if self.antithetic:\n",
        "      assert (self.popsize & 2), \"Population size must be even\"\n",
        "      self.half_popsize = int(self.popsize / 2)\n",
        "\n",
        "    self.reward = np.zeros(self.popsize)\n",
        "    self.mu = np.zeros(self.num_params)\n",
        "    self.best_mu = np.zeros(self.num_params)\n",
        "    self.best_reward = 0\n",
        "    self.first_interation = True\n",
        "    self.forget_best = forget_best\n",
        "\n",
        "  def rms_stdev(self):\n",
        "    sigma = self.sigma\n",
        "    return np.mean(np.sqrt(sigma*sigma))\n",
        "\n",
        "  def ask(self):\n",
        "    '''returns a list of parameters'''\n",
        "    # antithetic sampling\n",
        "    if self.antithetic:\n",
        "      self.epsilon_half = np.random.randn(self.half_popsize, self.num_params)\n",
        "      self.epsilon = np.concatenate([self.epsilon_half, - self.epsilon_half])\n",
        "    else:\n",
        "      self.epsilon = np.random.randn(self.popsize, self.num_params)\n",
        "\n",
        "    indices = np.random.choice(np.arange(self.epsilon.size), replace=False,\n",
        "                                   size=int(self.epsilon.size * 0.95))\n",
        "    self.epsilon[np.unravel_index(indices, self.epsilon.shape)] = 0\n",
        "\n",
        "    self.solutions = self.mu.reshape(1, self.num_params) + self.epsilon * self.sigma\n",
        "\n",
        "    return self.solutions\n",
        "\n",
        "  def tell(self, reward):\n",
        "    # input must be a numpy float array\n",
        "    assert(len(reward) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
        "\n",
        "    idx = np.argsort(reward)[::-1]\n",
        "\n",
        "    best_reward = reward[idx[0]]\n",
        "    best_mu = self.solutions[idx[0]]\n",
        "\n",
        "    self.curr_best_reward = best_reward\n",
        "    self.curr_best_mu = best_mu\n",
        "\n",
        "    if self.first_interation:\n",
        "      self.first_interation = False\n",
        "      self.best_reward = self.curr_best_reward\n",
        "      self.best_mu = best_mu\n",
        "    else:\n",
        "      if self.forget_best or (self.curr_best_reward > self.best_reward):\n",
        "        self.best_mu = best_mu\n",
        "        self.best_reward = self.curr_best_reward\n",
        "\n",
        "    # main bit:\n",
        "    # standardize the rewards to have a gaussian distribution\n",
        "    normalized_reward = (reward - np.mean(reward)) / np.std(reward)\n",
        "    self.mu += self.learning_rate/(self.popsize*self.sigma)*np.dot(self.epsilon.T, normalized_reward)\n",
        "\n",
        "    # adjust sigma according to the adaptive sigma calculation\n",
        "    if (self.sigma > self.sigma_limit):\n",
        "      self.sigma *= self.sigma_decay\n",
        "\n",
        "    if (self.learning_rate > self.learning_rate_limit):\n",
        "      self.learning_rate *= self.learning_rate_decay\n",
        "\n",
        "  def done(self):\n",
        "    return False\n",
        "\n",
        "  def current_param(self):\n",
        "    return self.curr_best_mu\n",
        "\n",
        "  def best_param(self):\n",
        "    return self.best_mu\n",
        "\n",
        "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
        "    return (self.best_mu, self.best_reward, self.curr_best_reward, self.sigma)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyt8JFLN8ANT"
      },
      "source": [
        "Args = namedtuple('Args', ['batch_size', 'test_batch_size', 'epochs', 'lr', 'cuda', 'seed', 'log_interval'])"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0N3J9Xi8ANm"
      },
      "source": [
        "args = Args(batch_size=1000, test_batch_size=1000, epochs=30, lr=0.001, cuda=True, seed=0, log_interval=10)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lU1jvbi8AOO"
      },
      "source": [
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "  torch.cuda.manual_seed(args.seed)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_ka-5yI8AOm"
      },
      "source": [
        "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  datasets.MNIST('MNIST_data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "  batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "valid_loader = train_loader\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  datasets.MNIST('MNIST_data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "  batch_size=args.batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNNC4qzd8AO0"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.num_filter1 = 8\n",
        "    self.num_filter2 = 16\n",
        "    self.num_padding = 2\n",
        "    # input is 28x28\n",
        "    # padding=2 for same padding\n",
        "    self.conv1 = nn.Conv2d(1, self.num_filter1, 5, padding=self.num_padding)\n",
        "    # feature map size is 14*14 by pooling\n",
        "    # padding=2 for same padding\n",
        "    self.conv2 = nn.Conv2d(self.num_filter1, self.num_filter2, 5, padding=self.num_padding)\n",
        "    # feature map size is 7*7 by pooling\n",
        "    self.fc = nn.Linear(self.num_filter2*7*7, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "    x = x.view(-1, self.num_filter2*7*7)   # reshape Variable\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OQnsskg8APD"
      },
      "source": [
        "NPOPULATION = 101\n",
        "weight_decay_coef = 0.1"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQZE8PEk8APT"
      },
      "source": [
        "'''\n",
        "models = []\n",
        "for i in range(NPOPULATION):\n",
        "  model = Net()\n",
        "  if args.cuda:\n",
        "    model.cuda()\n",
        "  model.eval()\n",
        "  models.append(model)\n",
        "'''\n",
        "\n",
        "model = Net()\n",
        "if args.cuda:\n",
        "  model.cuda()\n",
        "\n",
        "orig_model = copy.deepcopy(model)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWF4vvaY8APf",
        "outputId": "2d6fe349-370a-4507-fda5-3a9a73ba81c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# get init params\n",
        "orig_params = []\n",
        "model_shapes = []\n",
        "for param in orig_model.parameters():\n",
        "  p = param.data.cpu().numpy()\n",
        "  model_shapes.append(p.shape)\n",
        "  orig_params.append(p.flatten())\n",
        "orig_params_flat = np.concatenate(orig_params)\n",
        "NPARAMS = len(orig_params_flat)\n",
        "print(NPARAMS)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MixdXaTP8APy"
      },
      "source": [
        "def update_model(flat_param, model, model_shapes):\n",
        "  idx = 0\n",
        "  i = 0\n",
        "  for param in model.parameters():\n",
        "    delta = np.product(model_shapes[i])\n",
        "    block = flat_param[idx:idx+delta]\n",
        "    block = np.reshape(block, model_shapes[i])\n",
        "    i += 1\n",
        "    idx += delta\n",
        "    block_data = torch.from_numpy(block).float()\n",
        "    if args.cuda:\n",
        "      block_data = block_data.cuda()\n",
        "    param.data = block_data"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7cvvbTt8AQC"
      },
      "source": [
        "def evaluate(model, test_loader, print_mode=True, return_loss=False):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    if args.cuda:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "    data, target = Variable(data, volatile=True), Variable(target)\n",
        "    output = model(data)\n",
        "    test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
        "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  acc = correct.item() / len(test_loader.dataset)\n",
        "  \n",
        "  if print_mode:\n",
        "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
        "      test_loss, correct, len(test_loader.dataset),\n",
        "      100. * acc))\n",
        "  \n",
        "  if return_loss:\n",
        "    return test_loss\n",
        "  return acc"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlhTgR0G8AQR"
      },
      "source": [
        "\"\"\"\n",
        "es = SimpleES(NPARAMS,\n",
        "              popsize=NPOPULATION,\n",
        "              sigma_init=0.01,\n",
        "              sigma_decay=0.999,\n",
        "              sigma_alpha=0.2,\n",
        "              sigma_limit=0.001,\n",
        "              elite_ratio=0.1,\n",
        "              average_baseline=False,\n",
        "              forget_best=True\n",
        "             )\n",
        "\"\"\"\n",
        "es = OpenES(NPARAMS,\n",
        "              popsize=NPOPULATION,\n",
        "              sigma_init=0.01,\n",
        "              sigma_decay=0.999,\n",
        "              sigma_limit=0.01,\n",
        "              forget_best=False,\n",
        "              learning_rate=0.001,\n",
        "              learning_rate_decay = 0.9999,\n",
        "              learning_rate_limit = 0.0001,\n",
        "             )"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxE8pVU38AQj"
      },
      "source": [
        "def worker(procnum, model, solution, data, target, send_end):\n",
        "  update_model(solution, model, model_shapes)\n",
        "  output = model(data)\n",
        "  loss = F.nll_loss(output, target)\n",
        "  reward = - loss.data[0]\n",
        "  send_end.send(reward)\n",
        "\n",
        "def batch_simulation(model_list, solutions, data, target, process_count):\n",
        "  jobs = []\n",
        "  pipe_list = []\n",
        "\n",
        "  for i in range(process_count):\n",
        "    recv_end, send_end = mp.Pipe(False)\n",
        "    p = mp.Process(target=worker, args=(i, model_list[i], solutions[i], data, target, send_end))\n",
        "    jobs.append(p)\n",
        "    pipe_list.append(recv_end)\n",
        "\n",
        "  for p in jobs:\n",
        "    p.start()\n",
        "\n",
        "  for p in jobs:\n",
        "    p.join()\n",
        "\n",
        "  result_list = [x.recv() for x in pipe_list]\n",
        "  return np.array(result_list)\n",
        "\n",
        "\n",
        "def batch_simulation_sequential(model_list, solutions, data, target, process_count):\n",
        "  result_list = []\n",
        "  for i in range(process_count):\n",
        "    update_model(solutions[i], model_list[i], model_shapes)\n",
        "    output = model_list[i](data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    reward = - loss.data[0]\n",
        "    result_list.append(reward)\n",
        "  return np.array(result_list)\n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QSs-Cs08AQw"
      },
      "source": [
        "#'''\n",
        "best_valid_acc = 0\n",
        "training_log = []\n",
        "for epoch in range(1, 10*args.epochs + 1):\n",
        "\n",
        "  # train loop\n",
        "  model.eval()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    if args.cuda:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    \n",
        "    solutions = es.ask()\n",
        "    reward = np.zeros(es.popsize)\n",
        "    \n",
        "    for i in range(es.popsize):\n",
        "      update_model(solutions[i], model, model_shapes)\n",
        "      output = model(data)\n",
        "      loss = F.nll_loss(output, target)\n",
        "      reward[i] = - loss.item()\n",
        "\n",
        "    best_raw_reward = reward.max()\n",
        "    #reward = compute_centered_ranks(reward)\n",
        "    l2_decay = compute_weight_decay(weight_decay_coef, solutions)\n",
        "    reward += l2_decay\n",
        "\n",
        "    es.tell(reward)\n",
        "\n",
        "    result = es.result()\n",
        "    \n",
        "    if (batch_idx % 5 == 0):\n",
        "      print(epoch, batch_idx, best_raw_reward, result[0].mean(), result[3])\n",
        "\n",
        "  curr_solution = es.current_param()\n",
        "  update_model(curr_solution, model, model_shapes)\n",
        "\n",
        "  valid_acc = evaluate(model, valid_loader, print_mode=False)\n",
        "  training_log.append([epoch, valid_acc])\n",
        "  print('valid_acc', valid_acc * 100.)\n",
        "  if valid_acc >= best_valid_acc:\n",
        "    best_valid_acc = valid_acc\n",
        "    best_model = copy.deepcopy(model)\n",
        "    print('best valid_acc', best_valid_acc * 100.)\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH1_M6ZA8AQ-"
      },
      "source": [
        "evaluate(best_model, valid_loader, print_mode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCEy2XYu8ARK"
      },
      "source": [
        "evaluate(best_model, test_loader, print_mode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H_mBANt8ART"
      },
      "source": [
        "evaluate(best_model, train_loader, print_mode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-0viJ8T8ARg"
      },
      "source": [
        "update_model(es.best_param(), model, model_shapes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC0u7aGz8ARv"
      },
      "source": [
        "evaluate(model, valid_loader, print_mode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgJu9rez8ASA"
      },
      "source": [
        "evaluate(model, test_loader, print_mode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dms1t84h8ASK"
      },
      "source": [
        "evaluate(model, train_loader, print_mode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkgCCe878ASV"
      },
      "source": [
        "update_model(es.current_param(), model, model_shapes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH-KNlsw8ASf"
      },
      "source": [
        "evaluate(model, valid_loader, print_mode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYWuGPKU8ASv"
      },
      "source": [
        "evaluate(model, test_loader, print_mode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDmDYJ8h8AS6"
      },
      "source": [
        "evaluate(model, train_loader, print_mode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPCEGnW-8ATG"
      },
      "source": [
        "eval_acc = evaluate(best_model, test_loader)\n",
        "print('final test acc', eval_acc * 100.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhFOPcRt8ATP"
      },
      "source": [
        "param_count = 0\n",
        "for param in model.parameters():\n",
        "  print(param.data.shape)\n",
        "  param_count += np.product(param.data.shape)\n",
        "print(param_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8rULIOb8ATY"
      },
      "source": [
        "orig_params = []\n",
        "for param in orig_model.parameters():\n",
        "  orig_params.append(param.data.cpu().numpy().flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPlHraEI8ATj"
      },
      "source": [
        "orig_params_flat = np.concatenate(orig_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PryxPa98ATu"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Nt1-HU8AT2"
      },
      "source": [
        "_ = plt.hist(orig_params_flat, bins=200)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb1f6jg38AT-"
      },
      "source": [
        "final_params = []\n",
        "for param in best_model.parameters():\n",
        "  final_params.append(param.data.cpu().numpy().flatten())\n",
        "final_params_flat = np.concatenate(final_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em9vPmE58AUM"
      },
      "source": [
        "_ = plt.hist(final_params_flat, bins=200)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuxMvHNN8AUV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9K7DG-h8AUd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}